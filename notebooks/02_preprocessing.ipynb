{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Sentiment140\n",
    "\n",
    "In this notebook, we will process the dataset to prepare it for training.  \n",
    "This includes:\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Formatting the dataset for Word2Vec training\n",
    "\n",
    "The cleaned dataset from the EDA step will be used as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\", quiet=True)      \n",
    "nltk.download(\"punkt_tab\", quiet=True)  \n",
    "nltk.download(\"wordnet\", quiet=True)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Processed Dataset\n",
    "\n",
    "We load the cleaned dataset from the previous EDA step.  \n",
    "This dataset is stored in `data/processed/sentiment140_clean.csv`  \n",
    "and has already undergone stopword removal and basic text cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "      <td>switchfoot bummer shoulda david carr third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "      <td>kenichan dived many times ball managed save re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "      <td>whole body feels itchy fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "      <td>nationwideclass behaving mad cant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  tweet_length  \\\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...           115   \n",
       "1          0  is upset that he can't update his Facebook by ...           111   \n",
       "2          0  @Kenichan I dived many times for the ball. Man...            89   \n",
       "3          0    my whole body feels itchy and like its on fire             47   \n",
       "4          0  @nationwideclass no, it's not behaving at all....           111   \n",
       "\n",
       "                                          clean_text  \n",
       "0         switchfoot bummer shoulda david carr third  \n",
       "1  upset cant update facebook texting might cry r...  \n",
       "2  kenichan dived many times ball managed save re...  \n",
       "3                        whole body feels itchy fire  \n",
       "4                  nationwideclass behaving mad cant  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/sentiment140_clean.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization\n",
    "\n",
    "Each tweet is converted into a sequence of words using tokenization.  \n",
    "Since stopwords were already removed in EDA, we only split the text into individual words and apply **lemmatization** to standardize the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and lemmatizes the input text, removing punctuation and short words.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of lemmatized tokens.\n",
    "    \"\"\"\n",
    "    # Handle NaN or non-string values\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out non-alphabetic words and lemmatize\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "    \n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot bummer shoulda david carr third</td>\n",
       "      <td>[switchfoot, bummer, shoulda, david, carr, third]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan dived many times ball managed save re...</td>\n",
       "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feels itchy fire</td>\n",
       "      <td>[whole, body, feel, itchy, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass behaving mad cant</td>\n",
       "      <td>[nationwideclass, behaving, mad, cant]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  \\\n",
       "0         switchfoot bummer shoulda david carr third   \n",
       "1  upset cant update facebook texting might cry r...   \n",
       "2  kenichan dived many times ball managed save re...   \n",
       "3                        whole body feels itchy fire   \n",
       "4                  nationwideclass behaving mad cant   \n",
       "\n",
       "                                              tokens  \n",
       "0  [switchfoot, bummer, shoulda, david, carr, third]  \n",
       "1  [upset, cant, update, facebook, texting, might...  \n",
       "2  [kenichan, dived, many, time, ball, managed, s...  \n",
       "3                   [whole, body, feel, itchy, fire]  \n",
       "4             [nationwideclass, behaving, mad, cant]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens\"] = df[\"clean_text\"].apply(preprocess_text)\n",
    "\n",
    "df[[\"clean_text\", \"tokens\"]].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized dataset for the next step\n",
    "output_path = \"../data/processed/sentiment140_tokenized.csv\"\n",
    "df[['sentiment', 'clean_text', 'tokens']].to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
