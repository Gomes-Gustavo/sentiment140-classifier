{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec Model\n",
    "\n",
    "In this notebook, we train the **Word2Vec** model using the processed dataset.  \n",
    "Once trained, we will generate **tweet embeddings** and save them for training the LSTM model in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenized Dataset\n",
    "\n",
    "We load the tokenized dataset that was saved in the preprocessing step.\n",
    "Since lists were saved as strings, we convert them back to lists before training the Word2Vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/processed/sentiment140_tokenized.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(eval)\n",
    "\n",
    "sentences = df[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec Model\n",
    "\n",
    "We train a **Word2Vec** model using the tokenized tweets.  \n",
    "This model learns vector representations of words based on their context in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,  \n",
    "    window=3,         \n",
    "    min_count=3,     \n",
    "    workers=4,       \n",
    "    sg=1,           \n",
    "    epochs=10        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Word2Vec Model\n",
    "\n",
    "We save the trained model in the `models/` directory so it can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = \"../models/word2vec_model\"\n",
    "word2vec_model.save(word2vec_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Tweet Embeddings\n",
    "\n",
    "Each tweet is converted into a **numerical vector** by averaging the Word2Vec embeddings of its words.\n",
    "If a tweet has no words in the vocabulary, it will be represented by a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[switchfoot, bummer, shoulda, david, carr, third]</td>\n",
       "      <td>[-0.048860773, -0.0153961675, 0.16348921, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>[-0.18636408, 0.5754361, 0.17716423, 0.0499408...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
       "      <td>[-0.20103584, 0.40490478, 0.022874601, -0.1132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[whole, body, feel, itchy, fire]</td>\n",
       "      <td>[-0.46060118, 0.21935824, 0.012844193, 0.07566...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nationwideclass, behaving, mad, cant]</td>\n",
       "      <td>[-0.49469107, 0.49707192, 0.1308029, 0.0195472...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [switchfoot, bummer, shoulda, david, carr, third]   \n",
       "1  [upset, cant, update, facebook, texting, might...   \n",
       "2  [kenichan, dived, many, time, ball, managed, s...   \n",
       "3                   [whole, body, feel, itchy, fire]   \n",
       "4             [nationwideclass, behaving, mad, cant]   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.048860773, -0.0153961675, 0.16348921, 0.03...  \n",
       "1  [-0.18636408, 0.5754361, 0.17716423, 0.0499408...  \n",
       "2  [-0.20103584, 0.40490478, 0.022874601, -0.1132...  \n",
       "3  [-0.46060118, 0.21935824, 0.012844193, 0.07566...  \n",
       "4  [-0.49469107, 0.49707192, 0.1308029, 0.0195472...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tweet_vector(tokens, model):\n",
    "    \"\"\"\n",
    "    Converts a tweet into a numerical vector by averaging its Word2Vec word embeddings.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): List of tokenized words.\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Vector representation of the tweet.\n",
    "    \"\"\"\n",
    "    valid_tokens = [token for token in tokens if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "\n",
    "# Apply function to generate embeddings\n",
    "df[\"vector\"] = df[\"tokens\"].apply(lambda x: tweet_vector(x, word2vec_model))\n",
    "\n",
    "df[[\"tokens\", \"vector\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Preprocessed Dataset with Embeddings\n",
    "\n",
    "We save the dataset with tweet embeddings so it can be used in the next step:   training a **LSTM model** for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved\n"
     ]
    }
   ],
   "source": [
    "# Convert vector (NumPy array) to string with commas for safe CSV writing\n",
    "df[\"vector\"] = df[\"vector\"].apply(lambda v: str(list(v)))\n",
    "\n",
    "# Save the dataset\n",
    "output_path = \"../data/processed/sentiment140_vectors.csv\"\n",
    "df[[\"sentiment\", \"vector\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Dataset saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
